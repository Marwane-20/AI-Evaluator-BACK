# ğŸ§  AI Code Evaluation Backend

## ğŸ“‚ Structure

```
backend/
â”œâ”€â”€ .env                  # your API keys and URLs (not checked in)
â”œâ”€â”€ prompts.py            # list of prompt keys and test cases
â”œâ”€â”€ scoring.py            # functions to compute all criterion scores
â”œâ”€â”€ evaluate_models.py    # loads responses/*.json â†’ produces scores.json
â”œâ”€â”€ main.py               # (optional) command-line runner for single model
â”œâ”€â”€ responses/            # model response JSONs (one per AI service)
â”œâ”€â”€ requirements.txt      # liste of dependencies to download
â”œâ”€â”€ tests/                # unit tests for the responses and scores file
â”œâ”€â”€ api.py                # api exposure of our scores file
â””â”€â”€ scores.json           # output report of all models (generated)
```

## ğŸ”§ Prerequisites

- Python 3.10+

Create & activate a virtual environment:

```bash
python -m venv .venv
source .venv/bin/activate     # Linux / macOS
.\.venv\Scripts\Activate      # Windows PowerShell
```

Install dependencies:

```bash
pip install -r requirements.txt
```

Copy `example.env` â†’ `.env` and fill your API keys if you plan to regenerate `responses/` via actual API calls:

```ini
OPENAI_API_KEY=â€¦
MISTRAL_API_KEY=â€¦
GEMINI_API_KEY=â€¦
DEEPSEEK_API_KEY=â€¦
```

## ğŸš€ Usage

### 1. Evaluate All Models in Bulk

This script reads every `<model>.json` in `responses/`, runs all tests & scoring pipelines, and writes a combined report to `scores.json`:

```bash
python evaluate_models.py
```

Upon success youâ€™ll see:

```
âœ… Written /path/to/backend/scores.json
```

### 2. Inspect or Serve Scores

- Static JSON: open `scores.json` in your editor or serve it via your preferred web server.
- On-demand (optional): you can call `evaluate_models.py` to evaluate a single model:

```bash
python evaluate_models.py --model gemini
```

(Add argument parsing as needed.)

## ğŸ“‘ Response / Output Format

The generated `scores.json` has this structure:

```jsonc
{
  "<model_name>": {
    "<prompt_key>": {
      "present": true,
      "error": null,
      "test_counts": {
        "passed": 2,
        "total": 3
      },
      "overall_score": 4.3,
      "breakdown": {
        "correctness":        5.0,
        "robustness":         4.5,
        "linguistic_bias":    4.0,
        "performance":        4.2,
        "readability":        4.7,
        "security":           3.8,
        "comment_richness":   3.5,
        "syntax_diversity":   4.1,
        "logical_originality":3.9,
        "freedom_expression": 4.6
      }
    },
    ...
  },
  ...
}
```

- `correctness` & `robustness` come from your unit-test suite (`prompts.py â†’ TEST_CASES`).
- Other criteria are computed automatically via AST analysis, timing, linting, etc., in `scoring.py`.
- `overall_score` is a 1â€“5 aggregate (you may customize its formula).

## ğŸ› ï¸ Customization

- Add or refine test cases in `prompts.py:TEST_CASES`.
- Extend scoring functions in `scoring.py` (e.g., new criteria).
- Integrate real-time API calls to re-generate `responses/<model>.json` if needed (see `main.py`).
- Hook up to a web framework (FastAPI, Flask) to serve `/api/evaluate`.

## ğŸ“ Notes

- Ensure `responses/` contains one `.json` per model, with keys matching `PROMPT_KEYS`.
- If you only want to re-score (no API calls), you can skip providing any keys in `.env`.
- `scores.json` is overwritten on each run of `evaluate_models.py`.

## ğŸ¤ License & Contribution

Feel free to fork and extend! This backend is released under the MIT license. Any improvementsâ€”better test coverage, additional criteria, CI integrationâ€”are very welcome.
